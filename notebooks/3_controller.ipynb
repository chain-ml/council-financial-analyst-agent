{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bce510f-c7b4-46e0-a3cc-3ab2a265dbd0",
   "metadata": {},
   "source": [
    "# Create a Custom Controller\n",
    "\n",
    "The purposes of this notebook is to build the custom controller and filter used by the Financial Analyst Agent in notebook `4_financial_analyst_agent`.\n",
    "\n",
    "In council's [LLMController](https://github.com/chain-ml/council/blob/main/council/controllers/llm_controller.py), the `_execute` method uses an LLM to determine which chains to execute. In the custom controller, the `_execute` method is responsible for both selecting which chains to execute but to also to reformulate the user query for each chain in such a way that improves the chains' results. This is accomplished by using an LLM to reformulate the user query based on the conversational history between the user and the query. For example, imagine asking the agent for the financial performance of Microsoft in one query, and then following up with \"what is the stock price?\". Routing such a query to a chain such as Google search would not search for user's intent, which is finding out the stock price of Microsoft.\n",
    "\n",
    "In the custom filter, we use an LLM to write a comprehensive response to the user query by aggregating the results from all the successfully executed chains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8177c6f-368b-449c-b349-08dc677c21ed",
   "metadata": {},
   "source": [
    "## Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a28cc-d4f6-40a7-9bf1-16072616bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Tuple\n",
    "from string import Template\n",
    "\n",
    "from council.chains import Chain\n",
    "from council.contexts import AgentContext, ScoredChatMessage, ChatMessage, ChatMessageKind, LLMContext\n",
    "from council.controllers import LLMController, ExecutionUnit\n",
    "from council.filters import FilterBase\n",
    "from council.llm import LLMMessage, LLMBase\n",
    "\n",
    "import constants\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ed87c02-8642-4362-955c-312d95ae3e84",
   "metadata": {},
   "source": [
    "## Specifying constants used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd1f93-33f9-437f-8bb4-fe310aeda769",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Microsoft\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "268a1c98-0cc8-4f68-aaa1-5b54882d8179",
   "metadata": {},
   "source": [
    "## Prompts for Controller\n",
    "\n",
    "The `controller_system_prompt` and `controller_prompt` below are the prompts used for the LLM by controller. The `controller_prompt` performs the two-step process of understanding and classifying a user's query in a conversational context.  \n",
    "The query reformulation and chain selection tasks are performed in a single LLM call that requests the model to perform the tasks sequentially; referred to as Subtask 1 and Subtask 2, respectively.\n",
    "\n",
    "Subtask 1 focuses on understanding and improving the current user query by taking into consideration the context of the conversation, which includes the conversational history. The task is to refine the user query if needed by using context from the conversation history. For instance, if a user asked \"Who is the CEO of OpenAI?\" and then followed up with \"How old is he?\", the task would be to update the second query to \"How old is Sam Altman?\". However, the instructions also specify that if the query does not need updating, or if there's no relevant conversational history, the query should be left unchanged.\n",
    "\n",
    "Subtask 2 uses the updated query from Subtask 1 to identify the intent of the user, similar to council's `LLMController`. The LLM is presented with a list of categories (each with a name and a description), and is instructed to score each category out of 10 based on how well its description aligns with the intent of the updated query. If no category is relevant, the LLM should respond with 'unknown'.\n",
    "\n",
    "The prompt finished by describing the exact format to return the results of Subtask 1 and Subtask, to later be parsed by the controller.\n",
    "\n",
    "Note the occurence of words with a `$` prefix, such as `$user_query`. When building the controller's `_execute` method, we will load this prompt as a `Template` from the python string module, and words with the prefix denote variables that will be substituted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7213b1c-e2af-4c91-8710-fb2d17e6ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_system_prompt = \"You are an assistant responsible to identify the intent of the user.\"\n",
    "\n",
    "controller_prompt = \"\"\"\n",
    "Use the latest user query and the conversational history to identify the intent of the user. \n",
    "Break this task down into 2 subtasks. First perform subtask 1 and then subtask 2.\n",
    "\n",
    "Context for subtask 1:\n",
    "Conversational history:\n",
    "$conversational_history\n",
    "\n",
    "User query: $user_query\n",
    "\n",
    "Instructions for subtask 1:\n",
    "# Use the historical conversation to update the user query to better answer the user question\n",
    "# If the query does not need to be updated, do not update the query\n",
    "# If there is no conversational history, do not update the query\n",
    "# If the conversational history is not relevant to the query, do not update the query\n",
    "# See the below examples for how to update the user query\n",
    "************\n",
    "Example 1:\n",
    "Conversational History:\n",
    "User: Who is the CEO of OpenAI?\n",
    "Assistant: Sam Altman\n",
    "\n",
    "User Query: How old is he?\n",
    "\n",
    "Updated Query: How old is Sam Altman?\n",
    "************\n",
    "Example 2:\n",
    "Conversational History:\n",
    "User: Who is the CEO of OpenAI?\n",
    "Assistant: Sam Altman\n",
    "\n",
    "User Query: What is the price of Bitcoin?\n",
    "\n",
    "Updated Query: What is the price of Bitcoin?\n",
    "************\n",
    "\n",
    "Context for subtask 2: \n",
    "Categories are given as a name and a category (name: {name}, description: {description}):\n",
    "$answer_choices\n",
    "\n",
    "Instructions for subtask 2:\n",
    "# Use the updated query to identify the intent of the user\n",
    "# score categories out of 10 using there description\n",
    "# For each category, you will answer with {name};{score};{short justification}\"\n",
    "# The updated query should be identical for each category\n",
    "# Each response is provided on a new line\n",
    "# When no category is relevant, you will answer exactly with 'unknown'\n",
    "                                \n",
    "Your response should always be formatted like this:\n",
    "Subtask 1: {updated_query}\n",
    "---\n",
    "Subtask 2:\n",
    "{subtask2_results}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a39a18d5-f191-4610-af00-1d4c85f0d6c9",
   "metadata": {},
   "source": [
    "## Controller _execute method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea2a574-f482-4464-b19b-1f4008615fe8",
   "metadata": {},
   "source": [
    "One of the variables that will be substituted into the above prompt is the `conversational_history`. To do so, we first need to create a string representation of the conversational history from the context. \n",
    "\n",
    "The `build_chat_history` method below is responsible for iterating through the messages in chathistory in the context and building a string from messages that are either from the user or from the agent. The `max_history_len` argument, set to 4 by default, denotes the maximum number of messages to include in the conversational history. This was selected as the default with the assumption that more recent conversational history will be more relevant to determining the intent of the user's last message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fa146-336a-4d7e-ab1b-1e13876694f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_history(context: AgentContext, max_history_len: int = 4) -> str:\n",
    "    \"\"\"Format the chat history into a string that can be added to the prompt for the query reformulation model.\"\"\"\n",
    "    chat_history = \"\"\n",
    "    # Remove the user's most recent message from the chat history\n",
    "    message_history = list(context.chat_history.messages[:-1])\n",
    "    # Return no history if there are less than 2 messages\n",
    "    if len(message_history) < 1:\n",
    "        return \"No conversational history\"\n",
    "\n",
    "    for msg in message_history[-max_history_len:]:\n",
    "        if msg.is_of_kind(ChatMessageKind.User):\n",
    "            chat_history += f\"User: {msg.message}\\n\"\n",
    "        if msg.is_of_kind(ChatMessageKind.Agent):\n",
    "            chat_history += f\"Assistant: {msg.message}\\n\"\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3e3c206-5df1-4c21-be89-77e763ce9f39",
   "metadata": {},
   "source": [
    "After the prompt is created, sent to the LLM model and a response is received, we use the `parse_response` method below to extract the results of the subtasks. It returns a tuple of two strings: the first for the reformulated query and the second for the chains selected for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03527da0-299a-458a-8933-b5203300ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response: str) -> Tuple[str, str]:\n",
    "    \"\"\"Function to separate reformulated query and chain selection from LLM response.\"\"\"\n",
    "    query_reformulation_response = (\n",
    "        response.split(\"---\")[0].replace(\"Subtask 1:\", \"\").replace(\"Subtask 1: \", \"\").strip()\n",
    "    )\n",
    "    chain_selection_response = response.split(\"---\")[1].replace(\"Subtask 2:\", \"\").replace(\"Subtask 2: \", \"\").strip()\n",
    "\n",
    "    return query_reformulation_response, chain_selection_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "899a6c02-6954-4d92-8fbd-fca26dbb9847",
   "metadata": {},
   "source": [
    "The code below processes the selected chains from the LLM response by extracting their name and score, filtering for chains where the score exceeds the `response_threshold` (the minimum score required for the agent to execute the chain) and ordering by descending score. The `parse_response` function is a method of council's LLMController class for parsing the selected chains (subtask 2 for the custom controller). Since our custom controller will inherit from LLMController, it will not need to be redefined.\n",
    "\n",
    "An `ExecutionUnit` specifies a chain to be executed, and an initial state for the execution. We store the reformulated query in the `initial_state` variable so that each chain will have access to it during execution. \n",
    "\n",
    "We are also keeping a list of the chains that will be executed in the current iteration in the `chains_current_iteration` variable. This will be used later in the `select_responses` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc35ae3-04f7-4640-a2cf-01935066e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate reformulated query and chain selection from response\n",
    "query_reformulation_result, chain_selection_result = self.parse_response(response)\n",
    "# Create execution plan and provide reformulated query to each execution unit as its initial state\n",
    "parsed = [self._parse_line(line, self._chains) for line in chain_selection_result.splitlines()]\n",
    "filtered = [r.unwrap() for r in parsed if r.is_some() and r.unwrap()[1] > self._response_threshold]\n",
    "if (filtered is None) or (len(filtered) == 0):\n",
    "    return []\n",
    "\n",
    "filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "# List of chain names to be executed by agent in current iteration\n",
    "self.chains_current_iteration = [chain.name for chain, _ in filtered]\n",
    "result = [\n",
    "    ExecutionUnit(r[0], context.budget, initial_state=ChatMessage.chain(query_reformulation_result))\n",
    "    for r in filtered\n",
    "    if r is not None\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d05203-8c5a-4a96-b0d3-4dbbbf440ff7",
   "metadata": {},
   "source": [
    "We can now bring everything together for the controller's `_execute` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d710e0-7929-4984-8b3d-d0f645943bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _execute(self, context: AgentContext) -> List[ExecutionUnit]:\n",
    "    \"\"\"Generates an execution plan for the agent based on the provided context, chains, and budget.\"\"\"\n",
    "    answer_choices = \"\\n \".join([f\"name: {c.name}, description: {c.description}\" for c in self._chains])\n",
    "    # Load prompts for LLM and substitute parameters\n",
    "    system_prompt = \"You are an assistant responsible to identify the intent of the user.\"\n",
    "    \n",
    "    controller_prompt = Template(\"\"\"\n",
    "    Use the latest user query and the conversational history to identify the intent of the user. \n",
    "    Break this task down into 2 subtasks. First perform subtask 1 and then subtask 2.\n",
    "    \n",
    "    Context for subtask 1:\n",
    "    Conversational history:\n",
    "    $conversational_history\n",
    "    \n",
    "    User query: $user_query\n",
    "    \n",
    "    Instructions for subtask 1:\n",
    "    # Use the historical conversation to update the user query to better answer the user question\n",
    "    # If the query does not need to be updated, do not update the query\n",
    "    # If there is no conversational history, do not update the query\n",
    "    # If the conversational history is not relevant to the query, do not update the query\n",
    "    # See the below examples for how to update the user query\n",
    "    ************\n",
    "    Example 1:\n",
    "    Conversational History:\n",
    "    User: Who is the CEO of OpenAI?\n",
    "    Assistant: Sam Altman\n",
    "    \n",
    "    User Query: How old is he?\n",
    "    \n",
    "    Updated Query: How old is Sam Altman?\n",
    "    ************\n",
    "    Example 2:\n",
    "    Conversational History:\n",
    "    User: Who is the CEO of OpenAI?\n",
    "    Assistant: Sam Altman\n",
    "    \n",
    "    User Query: What is the price of Bitcoin?\n",
    "    \n",
    "    Updated Query: What is the price of Bitcoin?\n",
    "    ************\n",
    "    \n",
    "    Context for subtask 2: \n",
    "    Categories are given as a name and a category (name: {name}, description: {description}):\n",
    "    $answer_choices\n",
    "    \n",
    "    Instructions for subtask 2:\n",
    "    # Use the updated query to identify the intent of the user\n",
    "    # score categories out of 10 using there description\n",
    "    # For each category, you will answer with {name};{score};{short justification}\"\n",
    "    # The updated query should be identical for each category\n",
    "    # Each response is provided on a new line\n",
    "    # When no category is relevant, you will answer exactly with 'unknown'\n",
    "                                    \n",
    "    Your response should always be formatted like this:\n",
    "    Subtask 1: {updated_query}\n",
    "    ---\n",
    "    Subtask 2:\n",
    "    {subtask2_results}\n",
    "    \"\"\")\n",
    "    \n",
    "    user_prompt = controller_prompt.substitute(\n",
    "        conversational_history=self.build_chat_history(context),\n",
    "        user_query=context.chat_history.last_user_message.message,\n",
    "        answer_choices=answer_choices,\n",
    "    )\n",
    "    # Send messages and receive response from model\n",
    "    messages = [\n",
    "        LLMMessage.system_message(system_prompt),\n",
    "        LLMMessage.user_message(user_prompt),\n",
    "    ]\n",
    "    response = self._llm.post_chat_request(messages)[0]\n",
    "    logger.debug(f\"llm response: {response}\")\n",
    "    # Separate reformulated query and chain selection from response\n",
    "    query_reformulation_result, chain_selection_result = self.parse_response(response)\n",
    "    # Create execution plan and provide reformulated query to each execution unit as its initial state\n",
    "    parsed = [self.parse_line(line, self._chains) for line in chain_selection_result.splitlines()]\n",
    "    filtered = [r.unwrap() for r in parsed if r.is_some() and r.unwrap()[1] > self._response_threshold]\n",
    "    if (filtered is None) or (len(filtered) == 0):\n",
    "        return []\n",
    "\n",
    "    filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "    result = [\n",
    "        ExecutionUnit(r[0], context.budget, initial_state=ChatMessage.chain(query_reformulation_result))\n",
    "        for r in filtered\n",
    "        if r is not None\n",
    "    ]\n",
    "\n",
    "    return result[: self._top_k]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c362e5-ae03-4092-9d63-fc74834dd4ce",
   "metadata": {},
   "source": [
    "## Prompts for Filter class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "985db0d5-0236-4e13-9cd5-c4e0ff87b81a",
   "metadata": {},
   "source": [
    "The prompts below are used for the LLM call in the custom filter.\n",
    "\n",
    "The `filter_prompt` instructs the LLM to write a research report using the information provided by the successfully completed chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f9ec8-f3a3-428d-822c-344154d12a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_system_prompt = \"You are a financial analyst whose job is to write a research report answering the user query based on data about $company from different sources.\"\n",
    "\n",
    "filter_prompt = \"\"\"\n",
    "# Instructions\n",
    "- The provided context is a list of research data answering the user query from different sources.\n",
    "- Combine the following data from muliple sources into a single research report to answer the query.\n",
    "- Make sure to highlight any agreements or disagreements between different responses in the final response.\n",
    "- Explicitly state from which source different parts of the final response are from.\n",
    "\n",
    "# Context:\n",
    "$context\n",
    "\n",
    "# Query:\n",
    "$query\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fa6cbb0-40b5-4bd0-9743-4dbf8615cd85",
   "metadata": {},
   "source": [
    "## Selected responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdd00912-a58b-4128-9728-f53beba0460e",
   "metadata": {},
   "source": [
    "The code below creates the `context` for the LLM prompt. It iterates through the chain results (that will be provided by the Evaluator) and uses the chain name from each result to map to the chain description to explain the source of the response to the LLM. If a chain was not executed during the current iteration, then it is skipped during the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc852d-7cdb-488d-bb06-630f7a1c8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_llm_messages(self, context: AgentContext) -> List[LLMMessage]:\n",
    "    agent_messages = list(context.evaluation)\n",
    "    query = context.chat_history.last_user_message.message\n",
    "    context = \"\"\n",
    "    for message in agent_messages:\n",
    "        context += f\"Response: {message.message.message}\\n\\n\"\n",
    "\n",
    "    filter_prompt = Template(\"\"\"\n",
    "    # Instructions\n",
    "    - The provided context is a list of research data answering the user query from different sources.\n",
    "    - Combine the following data from multiple sources into a single research report to answer the query.\n",
    "    - Make sure to highlight any agreements or disagreements between different responses in the final response.\n",
    "    - Explicitly state from which source different parts of the final response are from.\n",
    "    \n",
    "    # Context:\n",
    "    $context\n",
    "    \n",
    "    # Query:\n",
    "    $query\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\")\n",
    "    prompt = filter_prompt.substitute(\n",
    "        context=context, query=query\n",
    "    )\n",
    "    return [\n",
    "        self._build_system_prompt(company=constants.COMPANY_NAME),\n",
    "        LLMMessage.user_message(prompt),\n",
    "    ]\n",
    "\n",
    "def _build_system_prompt(self, company: str) -> LLMMessage:\n",
    "    system_prompt = Template(\n",
    "        \"You are a financial analyst whose job is to write a research report answering the user query based on data about $company from different sources.\"\n",
    "    ).substitute(company=company)\n",
    "    return LLMMessage.system_message(system_prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "010bf042-986f-4aaf-a379-0a27b99b06de",
   "metadata": {},
   "source": [
    "We can now bring everything together for the filter's `_execute` method below. It returns the LLM responses that will be sent back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db0746e6ab9ad3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def _execute(self, context: AgentContext) -> List[ScoredChatMessage]:\n",
    "    \"\"\"Selects responses from the agent's context.\"\"\"\n",
    "    messages = self._build_llm_messages(context)\n",
    "    llm_response = self._llm.inner.post_chat_request(LLMContext.from_context(context, self._llm), messages=messages)\n",
    "\n",
    "    return [ScoredChatMessage(ChatMessage.agent(llm_response.first_choice), 1.0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3942674a-eed5-4dd0-ac39-f2af9d3bc615",
   "metadata": {},
   "source": [
    "## Complete Controller Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ad3ab7d53130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(LLMController):\n",
    "    \"\"\"\n",
    "    A controller that uses an LLM to decide the execution plan and\n",
    "    reformulates the user query based on the conversational history.\n",
    "\n",
    "    Based on LLMController: https://github.com/chain-ml/council/blob/main/council/controllers/llm_controller.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chains: List[Chain], llm: LLMBase, response_threshold: float):\n",
    "        \"\"\"\n",
    "        Initialize a new instance\n",
    "\n",
    "        Parameters:\n",
    "            llm (LLMBase): the instance of LLM to use\n",
    "            response_threshold (float): a minimum threshold to select a response from its score\n",
    "        \"\"\"\n",
    "        super().__init__(chains, llm, response_threshold)\n",
    "\n",
    "    def _execute(self, context: AgentContext) -> List[ExecutionUnit]:\n",
    "        \"\"\"Generates an execution plan for the agent based on the provided context, chains, and budget.\"\"\"\n",
    "        response = self._call_llm(context)\n",
    "        # Separate reformulated query and chain selection from response\n",
    "        query_reformulation_result, chain_selection_result = self.parse_response(response)\n",
    "        # Create execution plan and provide reformulated query to each execution unit as its initial state\n",
    "        parsed = [self._parse_line(line, self._chains) for line in chain_selection_result.splitlines()]\n",
    "        filtered = [r.unwrap() for r in parsed if r.is_some() and r.unwrap()[1] > self._response_threshold]\n",
    "        if (filtered is None) or (len(filtered) == 0):\n",
    "            return []\n",
    "\n",
    "        filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "        result = [\n",
    "            ExecutionUnit(r[0], context.budget, initial_state=ChatMessage.chain(query_reformulation_result))\n",
    "            for r in filtered\n",
    "            if r is not None\n",
    "        ]\n",
    "\n",
    "        return result[: self._top_k]\n",
    "\n",
    "    def _build_llm_messages(self, context):\n",
    "        answer_choices = \"\\n \".join([f\"name: {c.name}, description: {c.description}\" for c in self._chains])\n",
    "        # Load prompts for LLM and substitute parameters\n",
    "        system_prompt = \"You are an assistant responsible to identify the intent of the user.\"\n",
    "        controller_prompt = Template(\"\"\"\n",
    "        Use the latest user query and the conversational history to identify the intent of the user. \n",
    "        Break this task down into 2 subtasks. First perform subtask 1 and then subtask 2.\n",
    "        \n",
    "        Context for subtask 1:\n",
    "        Conversational history:\n",
    "        $conversational_history\n",
    "        \n",
    "        User query: $user_query\n",
    "        \n",
    "        Instructions for subtask 1:\n",
    "        # Use the historical conversation to update the user query to better answer the user question\n",
    "        # If the query does not need to be updated, do not update the query\n",
    "        # If there is no conversational history, do not update the query\n",
    "        # If the conversational history is not relevant to the query, do not update the query\n",
    "        # See the below examples for how to update the user query\n",
    "        ************\n",
    "        Example 1:\n",
    "        Conversational History:\n",
    "        User: Who is the CEO of OpenAI?\n",
    "        Assistant: Sam Altman\n",
    "        \n",
    "        User Query: How old is he?\n",
    "        \n",
    "        Updated Query: How old is Sam Altman?\n",
    "        ************\n",
    "        Example 2:\n",
    "        Conversational History:\n",
    "        User: Who is the CEO of OpenAI?\n",
    "        Assistant: Sam Altman\n",
    "        \n",
    "        User Query: What is the price of Bitcoin?\n",
    "        \n",
    "        Updated Query: What is the price of Bitcoin?\n",
    "        ************\n",
    "        \n",
    "        Context for subtask 2: \n",
    "        Categories are given as a name and a category (name: {name}, description: {description}):\n",
    "        $answer_choices\n",
    "        \n",
    "        Instructions for subtask 2:\n",
    "        # Use the updated query to identify the intent of the user\n",
    "        # score categories out of 10 using there description\n",
    "        # For each category, you will answer with {name};{score};short justification\"\n",
    "        # The updated query should be identical for each category\n",
    "        # Each response is provided on a new line\n",
    "        # When no category is relevant, you will answer exactly with 'unknown'\n",
    "                                        \n",
    "        Your response should always be formatted like this:\n",
    "        Subtask 1: {updated_query}\n",
    "        ---\n",
    "        Subtask 2:\n",
    "        {subtask2_results}\n",
    "        \"\"\")\n",
    "        user_prompt = controller_prompt.substitute(\n",
    "            conversational_history=self.build_chat_history(context),\n",
    "            user_query=context.chat_history.last_user_message.message,\n",
    "            answer_choices=answer_choices,\n",
    "        )\n",
    "        # Send messages and receive response from model\n",
    "        messages = [\n",
    "            LLMMessage.system_message(system_prompt),\n",
    "            LLMMessage.user_message(user_prompt),\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "    @staticmethod\n",
    "    def build_chat_history(context: AgentContext, max_history_len: int = 4) -> str:\n",
    "        \"\"\"Format the chat history into a string that can be added to the prompt for the query reformulation model.\"\"\"\n",
    "        chat_history = \"\"\n",
    "        # Remove the user's most recent message from the chat history\n",
    "        message_history = list(context.chat_history.messages[:-1])\n",
    "\n",
    "        # Return no history if there are less than 2 messages\n",
    "        if len(message_history) < 1:\n",
    "            return \"No conversational history\"\n",
    "\n",
    "        for msg in message_history[-max_history_len:]:\n",
    "            if msg.is_of_kind(ChatMessageKind.User):\n",
    "                chat_history += f\"User: {msg.message}\\n\"\n",
    "            if msg.is_of_kind(ChatMessageKind.Agent):\n",
    "                chat_history += f\"Assistant: {msg.message}\\n\"\n",
    "\n",
    "        return chat_history\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_response(response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Function to separate reformulated query and chain selection from LLM response.\"\"\"\n",
    "        query_reformulation_response = (\n",
    "            response.split(\"---\")[0].replace(\"Subtask 1:\", \"\").replace(\"Subtask 1: \", \"\").strip()\n",
    "        )\n",
    "        chain_selection_response = response.split(\"---\")[1].replace(\"Subtask 2:\", \"\").replace(\"Subtask 2: \", \"\").strip()\n",
    "\n",
    "        return query_reformulation_response, chain_selection_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd039d66cc370a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Complete Filter Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db75dbc-e3bc-4763-98cc-fba712f7c99d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LLMFilter(FilterBase):\n",
    "\n",
    "    def __init__(self, llm: LLMBase):\n",
    "        super().__init__()\n",
    "        self._llm = self.new_monitor(\"llm\", llm)\n",
    "\n",
    "    def _execute(self, context: AgentContext) -> List[ScoredChatMessage]:\n",
    "        \"\"\"Selects responses from the agent's context.\"\"\"\n",
    "        messages = self._build_llm_messages(context)\n",
    "        llm_response = self._llm.inner.post_chat_request(LLMContext.from_context(context, self._llm), messages=messages)\n",
    "\n",
    "        return [ScoredChatMessage(ChatMessage.agent(llm_response.first_choice), 1.0)]\n",
    "\n",
    "    def _build_llm_messages(self, context) -> List[LLMMessage]:\n",
    "        agent_messages = context.evaluation\n",
    "        query = context.chat_history.last_user_message.message\n",
    "        context = \"\"\n",
    "        for message in agent_messages:\n",
    "            context += f\"Response: {message.message.message}\\n\\n\"\n",
    "\n",
    "        filter_prompt = Template(\"\"\"\n",
    "        # Instructions\n",
    "        - The provided context is a list of research data answering the user query from different sources.\n",
    "        - Combine the following data from multiple sources into a single research report to answer the query.\n",
    "        - Make sure to highlight any agreements or disagreements between different responses in the final response.\n",
    "        - Explicitly state from which source different parts of the final response are from.\n",
    "        \n",
    "        # Context:\n",
    "        $context\n",
    "        \n",
    "        # Query:\n",
    "        $query\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        prompt = filter_prompt.substitute(\n",
    "            context=context, query=query\n",
    "        )\n",
    "        return [\n",
    "            self._build_system_prompt(company=constants.COMPANY_NAME),\n",
    "            LLMMessage.user_message(prompt),\n",
    "        ]\n",
    "\n",
    "    def _build_system_prompt(self, company: str) -> LLMMessage:\n",
    "        system_prompt = Template(\n",
    "            \"You are a financial analyst whose job is to write a research report answering the user query based on data about $company from different sources.\"\n",
    "        ).substitute(company=company)\n",
    "        return LLMMessage.system_message(system_prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f0af3c9-aec3-4d7e-b2cf-57b8ec0fa790",
   "metadata": {},
   "source": [
    "In the [next part](./4_financial_analyst_agent.ipynb), we will put everything together, add a few new components, and complete our financial analyst.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
