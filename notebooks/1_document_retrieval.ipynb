{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010b1968-06ea-4b5c-a83f-972629695460",
   "metadata": {},
   "source": [
    "# Create a Document Retrieval Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaf0ff-bd9a-42ea-a1f5-ccc5ff49dfa9",
   "metadata": {},
   "source": [
    "**Note**: this tutorial assumes basic knowledge of `council`. For an overview of `council` basics, please review this intro [tutorial](https://colab.research.google.com/drive/1yQ1YB3KYG7LWmQAt5N1l7ok2xM7NCcnA#scrollTo=comW1H1s-617). \n",
    "\n",
    "In this tutorial, we will be using `council` to build a document retrieval agent that can answer questions about Microsoft's 2022 10-K report, a comprehensive report filed annually by publicly-traded companies about their financial performance.\n",
    "\n",
    "In many real-world scenarios, LLMs might not have enough contextual information to answer certain types of queries. For instance, if a query pertains to very recent or rapidly changing data (like today's weather or latest stock prices), it falls outside the scope of a pre-trained LLM, which has been trained on a fixed, static corpus of text and does not inherently know about events in the world after its training data was collected. This applies to OpenAI's gpt-4 and gpt-3.5 models that have a knowledge cutoff date of September 2021 and would not be able to answer questions using information found in Microsoft's 2022 documents.\n",
    "\n",
    "This is where augmenting an LLM with external data can be beneficial. By adding up-to-date information fetched from external data sources to the LLM prompt as context, we can make the responses from the LLM more relevant and accurate.\n",
    "\n",
    "When we augment an LLM with external data, it can be beneficial to represent this external data in a way that allows for efficient search and retrieval. One common approach is to convert the data into a series of vectors, and store these in a vector index. A vector index is a data structure used to optimize the lookup of vectors in high-dimensional spaces. The vector representation allows us to perform similarity searches: given a query vector, we can find the most similar vectors in the index, which correspond to the most relevant pieces of data. This can significantly speed up data retrieval and make the augmentation process more efficient, especially when dealing with large volumes of data.\n",
    "\n",
    "We will be using `LLamaIndex`, a framework for augmenting LLMs with external data, for creating a vector index.\n",
    "- LLamaIndex [github](https://github.com/jerryjliu/llama_index)\n",
    "- LlamaIndex [documentation](https://gpt-index.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "The process will include the following steps:\n",
    "1) Extracting the text from Microsoft's 10-K saved as a pdf\n",
    "2) Splitting the text into chunks of a certain size\n",
    "3) Creating embeddings for each text chunk\n",
    "4) Storing the text chunks and their corresponding embeddings in a vector index\n",
    "5) Retrieving text chunks most similar to a user query based on semantic similarity\n",
    "\n",
    "Code from this notebook will be used to build the Financial Analyst Agent in `4_financial_analyst_agent`, but this notebook can also be executed to create a standalone search agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0488b96-d22a-41e1-9f5f-c94f66d1e214",
   "metadata": {},
   "source": [
    "## Install required libraries\n",
    "Install council and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f11bc-6316-4139-87f9-ffa5239a222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800fa8cc-0c71-4056-b127-258751d098e9",
   "metadata": {},
   "source": [
    "## Import the required modules\n",
    "Import the required modules from the Council framework, supporting frameworks such as LlamaIndex and set the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6e4fa8c-61de-4f37-aa7e-79df0e55d2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from string import Template\n",
    "\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from tiktoken import Encoding\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from council.skills import SkillBase, LLMSkill, PromptToMessages\n",
    "from council.runners import Budget\n",
    "from council.contexts import SkillContext, ChatMessage\n",
    "from council.llm import OpenAILLM, OpenAILLMConfiguration, LLMMessage\n",
    "from council.chains import Chain\n",
    "from council.controllers import BasicController\n",
    "from council.evaluators import BasicEvaluator\n",
    "from council.agents import Agent\n",
    "from council.contexts import AgentContext, ChatHistory\n",
    "from council.prompt import PromptBuilder\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ab704-a4c6-48eb-b7e2-a7a0e91c273e",
   "metadata": {},
   "source": [
    "## Specifying constants used in the notebook\n",
    "These parameters will dictate the behaviour of the document indexing and retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6093b4f3-85eb-42b0-b286-9749844f1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Microsoft\"\n",
    "COMPANY_TICKER = \"MSFT\"\n",
    "\n",
    "PDF_FILE_NAME = \"msft-10K-2022.pdf\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "ENCODING_NAME = \"cl100k_base\"\n",
    "MAX_CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 20\n",
    "CONTEXT_TOKEN_LIMIT = 3000\n",
    "NUM_RETRIEVED_DOCUMENTS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da11bb4-496a-414f-985e-d808deae5c5d",
   "metadata": {},
   "source": [
    "## Instantiating tokenizers\n",
    "\n",
    "A tokenizer is a component that breaks down text into smaller units called tokens. Tokenization is a crucial first step in text data preprocessing because machine learning models don't inherently understand text in its raw form. Instead, they require numerical input. Therefore, before feeding text into a model that generates vector representations / embeddings, the text must be tokenized.\n",
    "\n",
    "The tokenizer used for text chunking is the same tokenizer used for the selected embedding model from `sentence-transformers`. We use the it to split the document into chunks small enough to use with the embedding model. We create the ChunkingTokenizer class to wrap the tokenizer we load from `transformers` with the required methods for it to be used directly in the `LlamaIndex` text splitter `TokenTextSplitter`.\n",
    "\n",
    "The tokenizer for the OpenAI LLM will be used to count the number of tokens from the retrieved document chunks that we are adding to the model input to ensure we do not go over the model's token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc7a072-856b-40ce-a713-76eff591f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkingTokenizer:\n",
    "    \"\"\"Tokenizer for chunking document data for creation of embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "# Instantiate tokenizer for chunking\n",
    "chunking_tokenizer = ChunkingTokenizer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Instantiate tokenizer for OpenAI LLM\n",
    "llm_tokenizer = tiktoken.get_encoding(ENCODING_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a209bc-0299-4227-b651-e568729ed839",
   "metadata": {},
   "source": [
    "## Split the text\n",
    "We set up the TokenTextSplitter with the chunking tokenizer and with other set constants values, such as the maximum chunk size. A maximum chunk size of 256 is used because that is the maximum number of tokens we input into the `all-MiniLM-L6-v2` embedding model. We split the text based on paragraph and line breaks, and empty spaces.\n",
    "\n",
    "LlamaIndex splits the text by creating objects called a `Node` that contain the text from the document chunk and some additional data. This process is completed using `the SimpleNodeParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b81dc2-fc63-47ca-9b54-48120630d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate text splitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=MAX_CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    tokenizer=chunking_tokenizer,\n",
    "    separator=\"\\n\\n\",\n",
    "    backup_separators=[\"\\n\", \" \"])\n",
    "\n",
    "# Instantiate node parser\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2f04f-9a3f-4c79-8e5a-0573c1b2c206",
   "metadata": {},
   "source": [
    "## Create the Vector Index\n",
    "We have the end-to-end creation of the LlamaIndex vector index, where we extract the text from the pdf document, split it into nodes, calculate the embeddings for each node and store it into a vector index. Finally, we initialize the vector index as a retriever (that we can interact with to retrieve nodes based on semantic similarity) by specifying the number of most similar nodes to retrieve.\n",
    "\n",
    "**Note:** LlamaIndex requires a local model name to begin with the *local:* prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "159693fc-1f86-42c3-a0ad-9bae9415f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the embedding model and node parser\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=f\"local:{EMBEDDING_MODEL_NAME}\", node_parser=node_parser)\n",
    "\n",
    "# Extract the text from the pdf document\n",
    "documents = SimpleDirectoryReader(input_files=[PDF_FILE_NAME]).load_data()\n",
    "\n",
    "# Create the index by splitting text into nodes and calculating text embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Initialize index as retriever for top K most similar nodes\n",
    "index_retriever = index.as_retriever(similarity_top_k=NUM_RETRIEVED_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9cae2-0a99-4860-b806-15dcfcafc8d9",
   "metadata": {},
   "source": [
    "## Create the Document Retrieval Skill\n",
    "We first create a `Retriever` class that will interact with LlamaIndex to retrieve most similar documents (nodes) and process them into paragraphs of text that can be added to an LLM prompt.\n",
    "\n",
    "When a query comes in, it is also converted into a similar high-dimensional vector using the same `all-MiniLM-L6-v2` embedding model. The system then calculates the cosine similarity between this query vector and all the vectors in the index. The top NUM_RETRIEVED_DOCUMENTS results are then chosen based on these similarity scores. The documents whose vectors are the most similar to the query vector are considered the most relevant and are returned.\n",
    "\n",
    "We then create our `DocRetrievalSkill`, which inherits from council's `SkillBase`, that queries the `Retriever` with the last user message and returns the formatted document text chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961317c5-c76b-46eb-a489-3d56214c27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility class for document retrieval with LlamaIndex\n",
    "class Retriever:\n",
    "    def __init__(self, llm_tokenizer: Encoding, retriever: VectorIndexRetriever):\n",
    "        \"\"\"Class to retrieve text chunks from Llama Index and create context for LLM\"\"\"\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def retrieve_docs(self, query) -> str:\n",
    "        \"\"\"End-to-end functiont to retrieve most similar nodes and build the context\"\"\"\n",
    "        nodes = self.retriever.retrieve(query)\n",
    "        docs = self._extract_text(nodes)\n",
    "        context = self._build_context(docs)\n",
    "\n",
    "        return context\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_text(nodes: List[NodeWithScore]) -> List[str]:\n",
    "         \"\"\"Function to extract the text from the retrieved nodes\"\"\"\n",
    "        return [node.node.text for node in nodes]\n",
    "\n",
    "    def _build_context(self, docs: List[str]) -> str:\n",
    "        \"\"\"Function to build context for LLM by separating text chunks into paragraphs\"\"\"\n",
    "        context = \"\"\n",
    "        num_tokens = 0\n",
    "        for doc in docs:\n",
    "            doc += \"\\n\\n\"\n",
    "            num_tokens += len(self.llm_tokenizer.encode(doc))\n",
    "            if num_tokens <= CONTEXT_TOKEN_LIMIT:\n",
    "                context += doc\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "# Define document retrieval skill\n",
    "class DocRetrievalSkill(SkillBase):\n",
    "    \"\"\"Skill to retrieve documents and build context\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: Retriever):\n",
    "        super().__init__(name=\"document_retrieval\")\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def execute(self, context: SkillContext, budget: Budget) -> ChatMessage:\n",
    "        query = context.last_user_message.message\n",
    "        context = self.retriever.retrieve_docs(query)\n",
    "\n",
    "        return self.build_success_message(context)\n",
    "\n",
    "\n",
    "# Instantiate document retrieval skill\n",
    "doc_retrieval_skill = DocRetrievalSkill(Retriever(llm_tokenizer, index_retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0a1f7-56f2-48c0-96f2-347c644ac102",
   "metadata": {},
   "source": [
    "## Create the LLMSkill\n",
    "The `LLMSkill` is a skill provided by council that can make a call and return a response from an LLM model. \n",
    "We will inject the context created from the retrieved documents by the `DocRetrievalSkill` into the `LLMSkill`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809ce440-6338-4fae-a766-bc4bd6802bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI LLM configurations from environment variables\n",
    "config = OpenAILLMConfiguration.from_env()\n",
    "llm = OpenAILLM(config)\n",
    "\n",
    "# OpenAI LLM prompts\n",
    "SYSTEM_MESSAGE = \"You are a financial analyst whose job is to answer user questions about $company with the provided context.\"\n",
    "\n",
    "PROMPT = \"\"\"Use the following pieces of context to answer the query.\n",
    "If the answer is not provided in the context, do not make up an answer. Instead, respond that you do not know.\n",
    "\n",
    "CONTEXT:\n",
    "{{chain_history.last_message}}\n",
    "END CONTEXT.\n",
    "\n",
    "QUERY:\n",
    "{{chat_history.user.last_message}}\n",
    "END QUERY.\n",
    "\n",
    "YOUR ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "# Function used by the LLMSkill to add the document context and user message into LLM prompt\n",
    "def build_context_messages(context: SkillContext) -> List[LLMMessage]:\n",
    "    \"\"\"Context messages function for LLMSkill\"\"\"\n",
    "    context_message_prompt = PromptToMessages(prompt_builder=PromptBuilder(PROMPT))\n",
    "    return context_message_prompt.to_user_message(context)\n",
    "\n",
    "# Instantiate LLMSkill\n",
    "llm_skill = LLMSkill(\n",
    "    llm=llm,\n",
    "    system_prompt=Template(SYSTEM_MESSAGE).substitute(company=COMPANY_NAME),\n",
    "    context_messages=build_context_messages,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d7003-5ee8-4269-9b12-c241c0a400ab",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "The chain will use the `DocRetrievalSkill` followed by the `LLMSkill` to answer the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23b6c297-dcfc-4c84-81af-7fc929a4ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retrieval_chain = Chain(\n",
    "    name=\"doc_retrieval_chain\",\n",
    "    description=f\"Information from {COMPANY_NAME} ({COMPANY_TICKER}) 10-K from their 2022 fiscal year, a document that contain important updates for investors about company performance and operations\",\n",
    "    runners=[doc_retrieval_skill, llm_skill],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c36e41-98fb-438b-9a47-fd19b7377d9f",
   "metadata": {},
   "source": [
    "## Create the document retrieval agent\n",
    "The agent will use the document retrieval chain we created and the `BasicController` and `BasicEvaluator`, which will select our single chain and return the response to the user.\n",
    "\n",
    "Notebooks `3_controller` and `4_financial_analyst_agent` will demonstrate more complicated uses of the controller and evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eea8e03-2c4a-4b45-a8db-d103783389b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(controller=BasicController(), chains=[doc_retrieval_chain], evaluator=BasicEvaluator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750e5a1-e5cc-4b00-97fc-cba795b7996a",
   "metadata": {},
   "source": [
    "## Interact with the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66113448-4f99-4a8e-aa24-29adad3f821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The business segments in Microsoft are Productivity and Business Processes, Intelligent Cloud, and More Personal Computing.\n"
     ]
    }
   ],
   "source": [
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"What are the business segments in Microsoft?\")\n",
    "run_context = AgentContext(chat_history)\n",
    "\n",
    "result = agent.execute(run_context, Budget(60))\n",
    "\n",
    "print(result.best_message.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7461d-ec25-4f0e-b26a-2917b009b712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fb48f-e78e-4f2e-873a-9b6d9336372a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020375b-2cd2-4906-91ce-49aaa8678652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
