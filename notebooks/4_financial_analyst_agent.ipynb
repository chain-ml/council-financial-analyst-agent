{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b1abea-c60d-4520-b41c-11fea7dff88b",
   "metadata": {},
   "source": [
    "# Create a Financial Analyst Agent\n",
    "\n",
    "This tutorial combines the components developed in the `1_document_retrieval`, `2_google_search` and `3_controller` notebooks, along with several new components that will be introduced in this notebook, into a Financial Analyst Agent.\n",
    "\n",
    "This agent will be able to answer user questions from leveraging 3 unique sources of information:\n",
    "1) the Microsoft 2022 10-K financial statements\n",
    "2) Google Search and News\n",
    "3) Historical stock trading data \n",
    "\n",
    "Each of these sources of information will be accessible through running a specific Chain. Our custom controller will be responsible for selecting which data sources to use / which chains to execute and then to aggregate the results into a single response.\n",
    "\n",
    "The Microsoft financial statements chain will be based on the work in `1_document_retrieval`. The Google Search and News chain will be based on the work in `2_google_search`. The historical stock trading data chain will be based on a new skill using PandasAI, a Python library designed to make pandas dataframes conversational by allowing you to ask questions to your data in natural language. \n",
    "\n",
    "We will also build a custom evaluator that passes along information to the controller to help it aggregate the results from executed chains.\n",
    "\n",
    "This tutorial highlights the ability to create an Agent that can synthesize a response from multiple different sources, leveraging different technologies such as document similarity search and automatic code generation, and unifying every component using the `council` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e09466-e7b4-4bf8-b95e-b907419e036a",
   "metadata": {},
   "source": [
    "## Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba766839-da4b-4795-b934-a21f354364fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "print(f\"OPENAI_API_KEY set: {os.getenv('OPENAI_API_KEY', None) is not None}\")\n",
    "print(f\"GOOGLE_SEARCH_ENGINE_ID set: {os.getenv('GOOGLE_SEARCH_ENGINE_ID', None) is not None}\")\n",
    "print(f\"GOOGLE_API_KEY set: {os.getenv('GOOGLE_API_KEY', None) is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a222f14f6b2aec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from string import Template\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from tiktoken import Encoding\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "from pandasai import PandasAI\n",
    "from pandasai.llm.openai import OpenAI\n",
    "\n",
    "from council.skills import SkillBase, LLMSkill, PromptToMessages\n",
    "from council.contexts import SkillContext, Budget, LLMContext\n",
    "from council.llm import OpenAILLM, LLMMessage, LLMBase\n",
    "from council.filters import FilterBase\n",
    "from council.chains import Chain\n",
    "from council.controllers import LLMController, ExecutionUnit\n",
    "from council.agents import Agent\n",
    "from council.contexts import AgentContext, ScoredChatMessage, ChatMessage, ChatMessageKind\n",
    "from council.prompt import PromptBuilder\n",
    "from council.skills.google import GoogleSearchSkill, GoogleNewsSkill\n",
    "from council.runners import Parallel\n",
    "from council.evaluators import BasicEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621036d-7e25-4a9b-ba78-e661c769619b",
   "metadata": {},
   "source": [
    "## Specifying constants used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b925e74-1d68-4621-af31-eb4b8654f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Microsoft\"\n",
    "COMPANY_TICKER = \"MSFT\"\n",
    "\n",
    "DOC_AND_GOOGLE_RETRIEVAL_LLM = 'gpt-3.5-turbo'\n",
    "PANDAS_LLM = 'gpt-3.5-turbo'\n",
    "CONTROLLER_LLM = 'gpt-4'\n",
    "\n",
    "PDF_FILE_NAME = \"msft-10K-2022.pdf\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "ENCODING_NAME = \"cl100k_base\"\n",
    "MAX_CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 20\n",
    "CONTEXT_TOKEN_LIMIT = 3000\n",
    "NUM_RETRIEVED_DOCUMENTS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73953e4-a4fd-43f2-93e4-a130838d2515",
   "metadata": {},
   "source": [
    "## Controller\n",
    "\n",
    "The custom controller performs the two-step process of understanding and classifying a user's query in a conversational context when selecting a plan, and aggregates the responses from all successfully executed chains into a final response.\n",
    "\n",
    "See notebook `3_controller` for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e72b741462e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(LLMController):\n",
    "    \"\"\"\n",
    "    A controller that uses an LLM to decide the execution plan and\n",
    "    reformulates the user query based on the conversational history.\n",
    "\n",
    "    Based on LLMController: https://github.com/chain-ml/council/blob/main/council/controllers/llm_controller.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chains: List[Chain], llm: LLMBase, response_threshold: float):\n",
    "        \"\"\"\n",
    "        Initialize a new instance\n",
    "\n",
    "        Parameters:\n",
    "            llm (LLMBase): the instance of LLM to use\n",
    "            response_threshold (float): a minimum threshold to select a response from its score\n",
    "        \"\"\"\n",
    "        super().__init__(chains, llm, response_threshold)\n",
    "\n",
    "    def _execute(self, context: AgentContext) -> List[ExecutionUnit]:\n",
    "        \"\"\"Generates an execution plan for the agent based on the provided context, chains, and budget.\"\"\"\n",
    "        response = self._call_llm(context)\n",
    "        # Separate reformulated query and chain selection from response\n",
    "        query_reformulation_result, chain_selection_result = self.parse_response(response)\n",
    "        # Create execution plan and provide reformulated query to each execution unit as its initial state\n",
    "        parsed = [self._parse_line(line, self._chains) for line in chain_selection_result.splitlines()]\n",
    "        filtered = [r.unwrap() for r in parsed if r.is_some() and r.unwrap()[1] > self._response_threshold]\n",
    "        if (filtered is None) or (len(filtered) == 0):\n",
    "            return []\n",
    "\n",
    "        filtered.sort(key=lambda item: item[1], reverse=True)\n",
    "        result = [\n",
    "            ExecutionUnit(r[0], context.budget, initial_state=ChatMessage.chain(query_reformulation_result))\n",
    "            for r in filtered\n",
    "            if r is not None\n",
    "        ]\n",
    "\n",
    "        return result[: self._top_k]\n",
    "\n",
    "    def _build_llm_messages(self, context):\n",
    "        answer_choices = \"\\n \".join([f\"name: {c.name}, description: {c.description}\" for c in self._chains])\n",
    "        # Load prompts for LLM and substitute parameters\n",
    "        system_prompt = \"You are an assistant responsible to identify the intent of the user.\"\n",
    "        controller_prompt = Template(\"\"\"\n",
    "        Use the latest user query and the conversational history to identify the intent of the user. \n",
    "        Break this task down into 2 subtasks. First perform subtask 1 and then subtask 2.\n",
    "        \n",
    "        Context for subtask 1:\n",
    "        Conversational history:\n",
    "        $conversational_history\n",
    "        \n",
    "        User query: $user_query\n",
    "        \n",
    "        Instructions for subtask 1:\n",
    "        # Use the historical conversation to update the user query to better answer the user question\n",
    "        # If the query does not need to be updated, do not update the query\n",
    "        # If there is no conversational history, do not update the query\n",
    "        # If the conversational history is not relevant to the query, do not update the query\n",
    "        # See the below examples for how to update the user query\n",
    "        ************\n",
    "        Example 1:\n",
    "        Conversational History:\n",
    "        User: Who is the CEO of OpenAI?\n",
    "        Assistant: Sam Altman\n",
    "        \n",
    "        User Query: How old is he?\n",
    "        \n",
    "        Updated Query: How old is Sam Altman?\n",
    "        ************\n",
    "        Example 2:\n",
    "        Conversational History:\n",
    "        User: Who is the CEO of OpenAI?\n",
    "        Assistant: Sam Altman\n",
    "        \n",
    "        User Query: What is the price of Bitcoin?\n",
    "        \n",
    "        Updated Query: What is the price of Bitcoin?\n",
    "        ************\n",
    "        \n",
    "        Context for subtask 2: \n",
    "        Categories are given as a name and a category (name: {name}, description: {description}):\n",
    "        $answer_choices\n",
    "        \n",
    "        Instructions for subtask 2:\n",
    "        # Use the updated query to identify the intent of the user\n",
    "        # score categories out of 10 using there description\n",
    "        # For each category, you will answer with {name};{score};short justification\"\n",
    "        # The updated query should be identical for each category\n",
    "        # Each response is provided on a new line\n",
    "        # When no category is relevant, you will answer exactly with 'unknown'\n",
    "                                        \n",
    "        Your response should always be formatted like this:\n",
    "        Subtask 1: {updated_query}\n",
    "        ---\n",
    "        Subtask 2:\n",
    "        {subtask2_results}\n",
    "        \"\"\")\n",
    "        user_prompt = controller_prompt.substitute(\n",
    "            conversational_history=self.build_chat_history(context),\n",
    "            user_query=context.chat_history.last_user_message.message,\n",
    "            answer_choices=answer_choices,\n",
    "        )\n",
    "        # Send messages and receive response from model\n",
    "        messages = [\n",
    "            LLMMessage.system_message(system_prompt),\n",
    "            LLMMessage.user_message(user_prompt),\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "    @staticmethod\n",
    "    def build_chat_history(context: AgentContext, max_history_len: int = 4) -> str:\n",
    "        \"\"\"Format the chat history into a string that can be added to the prompt for the query reformulation model.\"\"\"\n",
    "        chat_history = \"\"\n",
    "        # Remove the user's most recent message from the chat history\n",
    "        message_history = list(context.chat_history.messages[:-1])\n",
    "\n",
    "        # Return no history if there are less than 2 messages\n",
    "        if len(message_history) < 1:\n",
    "            return \"No conversational history\"\n",
    "\n",
    "        for msg in message_history[-max_history_len:]:\n",
    "            if msg.is_of_kind(ChatMessageKind.User):\n",
    "                chat_history += f\"User: {msg.message}\\n\"\n",
    "            if msg.is_of_kind(ChatMessageKind.Agent):\n",
    "                chat_history += f\"Assistant: {msg.message}\\n\"\n",
    "\n",
    "        return chat_history\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_response(response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Function to separate reformulated query and chain selection from LLM response.\"\"\"\n",
    "        query_reformulation_response = (\n",
    "            response.split(\"---\")[0].replace(\"Subtask 1:\", \"\").replace(\"Subtask 1: \", \"\").strip()\n",
    "        )\n",
    "        chain_selection_response = response.split(\"---\")[1].replace(\"Subtask 2:\", \"\").replace(\"Subtask 2: \", \"\").strip()\n",
    "\n",
    "        return query_reformulation_response, chain_selection_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d9c2b-e3ff-4a10-9b7b-0332b1a0329f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LLMFilter(FilterBase):\n",
    "\n",
    "    def __init__(self, llm: LLMBase):\n",
    "        super().__init__()\n",
    "        self._llm = self.new_monitor(\"llm\", llm)\n",
    "\n",
    "    def _execute(self, context: AgentContext) -> List[ScoredChatMessage]:\n",
    "        \"\"\"Selects responses from the agent's context.\"\"\"\n",
    "        messages = self._build_llm_messages(context)\n",
    "        llm_response = self._llm.inner.post_chat_request(LLMContext.from_context(context, self._llm), messages=messages)\n",
    "\n",
    "        return [ScoredChatMessage(ChatMessage.agent(llm_response.first_choice), 1.0)]\n",
    "\n",
    "    def _build_llm_messages(self, context) -> List[LLMMessage]:\n",
    "        agent_messages = context.evaluation\n",
    "        query = context.chat_history.last_user_message.message\n",
    "        context = \"\"\n",
    "        for message in agent_messages:\n",
    "            context += f\"Response: {message.message.message}\\n\\n\"\n",
    "\n",
    "        filter_prompt = Template(\"\"\"\n",
    "        # Instructions\n",
    "        - The provided context is a list of research data answering the user query from different sources.\n",
    "        - Combine the following data from multiple sources into a single research report to answer the query.\n",
    "        - Make sure to highlight any agreements or disagreements between different responses in the final response.\n",
    "        - Explicitly state from which source different parts of the final response are from.\n",
    "        \n",
    "        # Context:\n",
    "        $context\n",
    "        \n",
    "        # Query:\n",
    "        $query\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        prompt = filter_prompt.substitute(\n",
    "            context=context, query=query\n",
    "        )\n",
    "        return [\n",
    "            self._build_system_prompt(company=COMPANY_NAME),\n",
    "            LLMMessage.user_message(prompt),\n",
    "        ]\n",
    "\n",
    "    def _build_system_prompt(self, company: str) -> LLMMessage:\n",
    "        system_prompt = Template(\n",
    "            \"You are a financial analyst whose job is to write a research report answering the user query based on data about $company from different sources.\"\n",
    "        ).substitute(company=company)\n",
    "        return LLMMessage.system_message(system_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb20500-965a-4b89-8125-4d45a29ad8b8",
   "metadata": {},
   "source": [
    "## Document Retrieval\n",
    "\n",
    "The below components will be used in the document retrieval chain used by the Financial Analyst Agent. They are based on the `1_document_retrieval` notebook.\n",
    "\n",
    "The key change we make here is in the `DocRetrievalSkill`, where the query is no longer the one sent by the user, but it is the reformulated query created by the custom controller. The Agent adds the initial state (which is where the controller stored the reformulated query) of the chain's ExecutionUnit to the chain's chat history when it begins its execution. Therefore, we can use `context.current.messages[-1].message` to select the most recent message in the chain context's chat history, which is the reformulated query added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5129fe8-0279-4b12-8fc3-b3f109192131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkingTokenizer:\n",
    "    \"\"\"Tokenizer for chunking document data for creation of embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "# Instantiate tokenizer for chunking\n",
    "chunking_tokenizer = ChunkingTokenizer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Instantiate tokenizer for OpenAI LLM\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=MAX_CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    tokenizer=chunking_tokenizer,\n",
    "    separator=\"\\n\\n\",\n",
    "    backup_separators=[\"\\n\", \" \"])\n",
    "\n",
    "# Instantiate text splitter\n",
    "llm_tokenizer = tiktoken.get_encoding(ENCODING_NAME)\n",
    "\n",
    "# Instantiate node parser\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "\n",
    "# Specify the embedding model and node parser\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=f\"local:{EMBEDDING_MODEL_NAME}\", node_parser=node_parser)\n",
    "\n",
    "# Extract the text from the pdf document\n",
    "documents = SimpleDirectoryReader(input_files=[\"msft-10K-2022.pdf\"]).load_data()\n",
    "\n",
    "# Create the index by splitting text into nodes and calculating text embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Initialize index as retriever for top K most similar nodes\n",
    "index_retriever = index.as_retriever(similarity_top_k=NUM_RETRIEVED_DOCUMENTS)\n",
    "\n",
    "# Define utility class for document retrieval with LlamaIndex\n",
    "class Retriever:\n",
    "    def __init__(self, llm_tokenizer: Encoding, retriever: VectorIndexRetriever):\n",
    "        \"\"\"Class to retrieve text chunks from Llama Index and create context for LLM\"\"\"\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def retrieve_docs(self, query) -> str:\n",
    "        \"\"\"End-to-end function to retrieve most similar nodes and build the context\"\"\"\n",
    "        nodes = self.retriever.retrieve(query)\n",
    "        docs = self._extract_text(nodes)\n",
    "        context = self._build_context(docs)\n",
    "\n",
    "        return context\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_text(nodes: List[NodeWithScore]) -> List[str]:\n",
    "        \"\"\"Function to extract the text from the retrieved nodes\"\"\"\n",
    "        return [node.node.text for node in nodes]\n",
    "\n",
    "    def _build_context(self, docs: List[str]) -> str:\n",
    "        \"\"\"Function to build context for LLM by separating text chunks into paragraphs\"\"\"\n",
    "        context = \"\"\n",
    "        num_tokens = 0\n",
    "        for doc in docs:\n",
    "            doc += \"\\n\\n\"\n",
    "            num_tokens += len(self.llm_tokenizer.encode(doc))\n",
    "            if num_tokens <= CONTEXT_TOKEN_LIMIT:\n",
    "                context += doc\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class DocRetrievalSkill(SkillBase):\n",
    "    \"\"\"Skill to retrieve documents and build context\"\"\"\n",
    "\n",
    "    def __init__(self, retriever: Retriever):\n",
    "        super().__init__(name=\"document_retrieval\")\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def execute(self, context: SkillContext) -> ChatMessage:\n",
    "        query = context.current.last_message.message\n",
    "        context = self.retriever.retrieve_docs(query)\n",
    "\n",
    "        return self.build_success_message(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62466253-5679-42c7-9b6e-f41cf9362d8b",
   "metadata": {},
   "source": [
    "## Google Search\n",
    "\n",
    "The below components will be used in the Google search chain used by the Financial Analyst Agent. They are based on the `2_google_search` notebook, with several key differences.\n",
    "\n",
    "\n",
    "The skill for Google Search is not council's GoogleSearchSkill, but rather we make a new skill `CustomGoogleSearchSkill` that skips execution if the required API keys are not set and, similar to the change in the `DocRetrievalSkill`, the query used for the search is the reformulated query created by the controller.\n",
    "\n",
    "Similarily, the `CustomGoogleNewsSkill` is created to use the controller's reformulated query for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706cbf8-5794-49a6-b567-e3d3e9527731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGoogleSearchSkill(GoogleSearchSkill):\n",
    "    \"\"\"\n",
    "    A skill that performs a Google search using the reformulated query from the controller.\n",
    "\n",
    "    Based on GoogleSearchSkill: https://github.com/chain-ml/council/blob/main/council/skills/google/google_search_skill.py\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, context: SkillContext) -> ChatMessage:\n",
    "        # Execute the skill only if the API keys required for Google Search are provided\n",
    "        if self.gs:\n",
    "            prompt = context.current.last_message\n",
    "            resp = self.gs.execute(query=prompt.message, nb_results=5)\n",
    "            response_count = len(resp)\n",
    "            if response_count > 0:\n",
    "                return self.build_success_message(\n",
    "                    f\"{self._name} {response_count} responses for {prompt.message}\", json.dumps([r.dict() for r in resp])\n",
    "                )\n",
    "            return self.build_error_message(\"no response\")\n",
    "        return self.build_error_message(\"API keys for Google Search not provided\")\n",
    "\n",
    "\n",
    "class CustomGoogleNewsSkill(GoogleNewsSkill):\n",
    "    \"\"\"\n",
    "    A skill that performs a Google News search using the reformulated query from the controller.\n",
    "    \n",
    "    Based on GoogleNewsSkill: https://github.com/chain-ml/council/blob/main/council/skills/google/google_news_skill.py\n",
    "    \"\"\"\n",
    "\n",
    "    def execute(self, context: SkillContext) -> ChatMessage:\n",
    "        prompt = context.current.last_message\n",
    "        resp = self.gn.execute(query=prompt.message, nb_results=5)\n",
    "        response_count = len(resp)\n",
    "        if response_count > 0:\n",
    "            return self.build_success_message(\n",
    "                f\"gnews {response_count} responses for {prompt.message}\", json.dumps([r.dict() for r in resp])\n",
    "            )\n",
    "        return self.build_error_message(\"no response\")\n",
    "\n",
    "class GoogleAggregator(SkillBase):\n",
    "    \"\"\"Skill to aggregate results from Google Search and Google News\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__(name=\"google_aggregator\")\n",
    "\n",
    "    def execute(self, context: SkillContext) -> ChatMessage:\n",
    "        gsearch_results = (\n",
    "            json.loads(context.current.last_message_from_skill(\"gsearch\").data)\n",
    "            if context.current.last_message_from_skill(\"gsearch\").is_ok\n",
    "            else []\n",
    "        )\n",
    "        gnews_results = (\n",
    "            json.loads(context.current.last_message_from_skill(\"gnews\").data)\n",
    "            if context.current.last_message_from_skill(\"gnews\").is_ok\n",
    "            else []\n",
    "        )\n",
    "        search_results = gsearch_results + gnews_results\n",
    "\n",
    "        context = \"\"\n",
    "        for result in search_results:\n",
    "            text = result.get(\"title\", \"\") + \" \" + result.get(\"snippet\", \"\") + \"\\n\\n\"\n",
    "            context += text\n",
    "\n",
    "        return self.build_success_message(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd865a-4ca4-4189-b03f-99a9c7ac1175",
   "metadata": {},
   "source": [
    "## PandasAI Skill\n",
    "\n",
    "The `PandasSkill` is a wrapper around [PandasAI](https://github.com/gventuri/pandas-ai/tree/main), a Python library that integrates generative AI capabilities into Pandas, making dataframes conversational.\n",
    "\n",
    "It loads a csv file containing stock price and volume data for Microsoft into a dataframe, uses an LLM to generate Python code to answer the user query, and then returns a natural language response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cba03-3173-45ea-903f-0ec38b816863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasSkill(SkillBase):\n",
    "    \"\"\"\n",
    "    Skill to converse with pandas Dataframe using PandasAI.\n",
    "    \n",
    "    PandasAI: https://github.com/gventuri/pandas-ai/tree/main\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_token, model):\n",
    "        super().__init__(name=\"pandas\")\n",
    "        self.llm = OpenAI(api_token=api_token, model=model)\n",
    "\n",
    "    def execute(self, context: SkillContext) -> ChatMessage:\n",
    "        query = context.current.last_message.message\n",
    "\n",
    "        df = pd.read_csv(\"MSFT.csv\")\n",
    "        pandas_ai = PandasAI(self.llm, conversational=True)\n",
    "\n",
    "        try:\n",
    "            response = pandas_ai(data_frame=df, prompt=query)\n",
    "            if \"Unfortunately, I was not able to answer your question, because of the following error:\" in response:\n",
    "                return self.build_error_message(response)\n",
    "            return self.build_success_message(response)\n",
    "        except Exception as e:\n",
    "            return self.build_error_message(f\"PandasAI failed due to following error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c26c9-6fa5-4b7e-98dd-772f50485ef4",
   "metadata": {},
   "source": [
    "# Initialize Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f6cbb-fcb4-4e21-acbe-74e5750c72dc",
   "metadata": {},
   "source": [
    "## Initialize Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be0d81-9280-40fc-955e-633a785fb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document retrieval skill\n",
    "doc_retrieval_skill = DocRetrievalSkill(Retriever(llm_tokenizer, index_retriever))\n",
    "\n",
    "# LLM Skill\n",
    "llm_skill_model = OpenAILLM.from_env(model=DOC_AND_GOOGLE_RETRIEVAL_LLM)\n",
    "\n",
    "SYSTEM_MESSAGE = \"You are a financial analyst whose job is to answer user questions about $company with the provided context.\"\n",
    "\n",
    "PROMPT = \"\"\"Use the following pieces of context to answer the query.\n",
    "If the answer is not provided in the context, do not make up an answer. Instead, respond that you do not know.\n",
    "\n",
    "CONTEXT:\n",
    "{{chain_history.last_message}}\n",
    "END CONTEXT.\n",
    "\n",
    "QUERY:\n",
    "{{chat_history.user.last_message}}\n",
    "END QUERY.\n",
    "\n",
    "YOUR ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "def build_context_messages(context: SkillContext) -> List[LLMMessage]:\n",
    "    \"\"\"Context messages function for LLMSkill\"\"\"\n",
    "    context_message_prompt = PromptToMessages(prompt_builder=PromptBuilder(PROMPT))\n",
    "    return context_message_prompt.to_user_message(context)\n",
    "\n",
    "\n",
    "llm_skill = LLMSkill(\n",
    "    llm=llm_skill_model,\n",
    "    system_prompt=Template(SYSTEM_MESSAGE).substitute(company=COMPANY_NAME),\n",
    "    context_messages=build_context_messages,\n",
    ")\n",
    "\n",
    "# Google search skills\n",
    "google_search_skill = CustomGoogleSearchSkill()\n",
    "google_news_skill = CustomGoogleNewsSkill()\n",
    "google_aggregator_skill = GoogleAggregator()\n",
    "\n",
    "pandas_skill = PandasSkill(api_token=os.getenv(\"OPENAI_API_KEY\"), model=PANDAS_LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027aef5-0194-4da5-80cd-287824d580b8",
   "metadata": {},
   "source": [
    "## Initialize Chains\n",
    "\n",
    "The Financial Analyst Agent will have 3 chains; one for document retrieval, one for searching with Google and one for analyzing a dataframe of historical stock price and trading data with LLM-generated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf8569-8bca-47e0-a8ce-efdda84f61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_retrieval_chain = Chain(\n",
    "    name=\"doc_retrieval_chain\",\n",
    "    description=f\"Information from {COMPANY_NAME} ({COMPANY_TICKER}) 10-K from their 2022 fiscal year, a document that contain important updates for investors about company performance and operations\",\n",
    "    runners=[doc_retrieval_skill, llm_skill],\n",
    ")\n",
    "\n",
    "search_chain = Chain(\n",
    "    name=\"search_chain\",\n",
    "    description=f\"Information about {COMPANY_NAME} ({COMPANY_TICKER}) using a Google search\",\n",
    "    runners=[\n",
    "        Parallel(google_search_skill, google_news_skill),\n",
    "        google_aggregator_skill,\n",
    "        llm_skill,\n",
    "    ],\n",
    ")\n",
    "\n",
    "pandas_chain = Chain(\n",
    "    name=\"pandas_chain\",\n",
    "    description=f\"{COMPANY_NAME} ({COMPANY_TICKER}) historical stock price and trading data information\",\n",
    "    runners=[pandas_skill],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c344d-1f44-4517-8188-cd7c118da5af",
   "metadata": {},
   "source": [
    "## Initialize Controller and Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e83b4-5fe5-42bc-adf1-d3d845ca38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing controller LLM\n",
    "controller_llm = OpenAILLM.from_env(model=CONTROLLER_LLM)\n",
    "controller = Controller([doc_retrieval_chain, search_chain, pandas_chain], controller_llm, response_threshold=5)\n",
    "\n",
    "evaluator = BasicEvaluator()\n",
    "filter = LLMFilter(controller_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a915e-d9bf-45fa-9cb6-36edc3469b4d",
   "metadata": {},
   "source": [
    "## Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edbc67-99b7-4c34-802e-48b934860f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(controller, evaluator, filter)\n",
    "run_context = AgentContext.from_user_message(\"How is the financial performance of Microsoft?\", Budget(600))\n",
    "result = agent.execute(run_context)\n",
    "print(result.best_message.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1e733-5c58-407b-9efa-a9c231077d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_context.chat_history.add_agent_message(result.best_message.message)\n",
    "run_context.chat_history.add_user_message(\"What are its business segments?\")\n",
    "result = agent.execute(run_context)\n",
    "print(result.best_message.message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
